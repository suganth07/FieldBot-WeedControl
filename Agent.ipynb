{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.types import Command\n",
    "import json\n",
    "import requests\n",
    "import json\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from ultralytics import YOLO\n",
    "import base64\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from langgraph.prebuilt import InjectedState\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import pyrealsense2 as rs\n",
    "from PIL import Image, ImageDraw\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatGroq(groq_api_key = \"your_api\",model_name = \"llama3-70b-8192\")\n",
    "\n",
    "# GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key='your_api')\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    weeds: Optional[list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools needed : \n",
    "\n",
    "1. Agent 1\n",
    "* Analyze image\n",
    "* Find coordinates\n",
    "* Make a plan\n",
    "\n",
    "2. Agent 2\n",
    "* Code the plan\n",
    "* Execute the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.types import Command\n",
    "from typing_extensions import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"analyze_image\")\n",
    "def analyze_image(state: Annotated[State, InjectedState]):\n",
    "    \"\"\"\n",
    "    Continuously capture video from RealSense camera, analyze each frame for weeds,\n",
    "    calculate their angles from the camera position, and display the results in real-time.\n",
    "    Ignores duplicates of the same weed and converts angles to servo-compatible format.\n",
    "    \n",
    "    Outputs data in structured format [{distance:value,angle:value}, {distance:value,angle:value}]\n",
    "    \"\"\"\n",
    "    # Fixed parameters\n",
    "    model_path = './runs/detect/train/weights/best.pt'\n",
    "    confidence_threshold = 0.5\n",
    "    \n",
    "    # Servo angle conversion parameters\n",
    "    camera_fov = 69  # Horizontal field of view for RealSense camera in degrees\n",
    "    servo_center = 90  # Center position for servo (typically 90 degrees)\n",
    "    \n",
    "    # Load YOLO model\n",
    "    print(\"Loading YOLO model...\")\n",
    "    model = YOLO(model_path)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Configure RealSense pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    \n",
    "    # Create an align object to align depth frames to color frames\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "    \n",
    "    # Start streaming\n",
    "    print(\"Starting RealSense camera stream...\")\n",
    "    pipeline.start(config)\n",
    "    print(\"Camera stream started\")\n",
    "    \n",
    "    # Track detected weeds to avoid duplicates\n",
    "    previously_detected_weeds = []\n",
    "    tracking_ttl = 15  # Frames to track a weed before considering it a new detection\n",
    "    tracking_distance_threshold = 50  # Pixel distance to consider weed the same\n",
    "    \n",
    "    # Store all unique weed angles\n",
    "    all_unique_angles = []\n",
    "    \n",
    "    # Store structured data for final output\n",
    "    structured_weed_data = []\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            \n",
    "            # Align the depth frame to color frame\n",
    "            aligned_frames = align.process(frames)\n",
    "            color_frame = aligned_frames.get_color_frame()\n",
    "            depth_frame = aligned_frames.get_depth_frame()\n",
    "            \n",
    "            if not color_frame or not depth_frame:\n",
    "                continue\n",
    "\n",
    "            # Get camera intrinsics for angle calculations\n",
    "            color_intrinsics = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "            \n",
    "            # Convert color frame to numpy array\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            \n",
    "            # Perform detection on the captured frame\n",
    "            results = model(color_image, conf=confidence_threshold, verbose=False)\n",
    "            \n",
    "            # Convert the image to PIL format for drawing\n",
    "            image_pil = Image.fromarray(color_image)\n",
    "            draw = ImageDraw.Draw(image_pil)\n",
    "            \n",
    "            # Create lists to store detection data\n",
    "            weed_coordinates = []\n",
    "            confidence_scores = []\n",
    "            weed_depths = []\n",
    "            weed_angles = []\n",
    "            current_detected_weeds = []\n",
    "            \n",
    "            # Process detection results\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    class_id = int(box.cls)\n",
    "                    if class_id == 1:  # Assuming class_id 1 is for weeds\n",
    "                        confidence = float(box.conf)\n",
    "                        x_min, y_min, x_max, y_max = box.xyxy[0].tolist()\n",
    "\n",
    "                        # Calculate the center of the bounding box\n",
    "                        center_x = int((x_min + x_max) / 2)\n",
    "                        center_y = int((y_min + y_max) / 2)\n",
    "\n",
    "                        # Get the depth value at the center of the bounding box\n",
    "                        depth = depth_frame.get_distance(center_x, center_y)  # Depth in meters\n",
    "                        \n",
    "                        # Skip if depth is zero (invalid)\n",
    "                        if depth <= 0:\n",
    "                            continue\n",
    "                            \n",
    "                        depth_mm = depth * 1000  # Convert to millimeters\n",
    "                        \n",
    "                        # Calculate angle from camera position\n",
    "                        point_3d = rs.rs2_deproject_pixel_to_point(color_intrinsics, [center_x, center_y], depth)\n",
    "                        \n",
    "                        # Calculate horizontal angle (from camera center)\n",
    "                        camera_angle = math.degrees(math.atan2(point_3d[0], point_3d[2]))\n",
    "                        vertical_angle = math.degrees(math.atan2(point_3d[1], point_3d[2]))\n",
    "                        \n",
    "                        \n",
    "                        # Create weed data structure for tracking\n",
    "                        current_weed = {\n",
    "                            \"center\": (center_x, center_y),\n",
    "                            \"depth\": depth_mm,\n",
    "                            \"camera_angle\": camera_angle,\n",
    "                            \"vertical_angle\": vertical_angle,\n",
    "                            \"confidence\": confidence,\n",
    "                            \"coordinates\": [x_min, y_min, x_max, y_max],\n",
    "                            \"ttl\": tracking_ttl,\n",
    "                            \"is_new\": True\n",
    "                        }\n",
    "                        \n",
    "                        # Check if this is a new weed or a previously seen one\n",
    "                        for prev_weed in previously_detected_weeds:\n",
    "                            prev_center = prev_weed[\"center\"]\n",
    "                            # Calculate distance between current detection and previous detection\n",
    "                            distance = np.sqrt((center_x - prev_center[0])*2 + (center_y - prev_center[1])*2)\n",
    "                            \n",
    "                            # If distance is small, it's likely the same weed\n",
    "                            if distance < tracking_distance_threshold:\n",
    "                                # Update TTL for the previous weed\n",
    "                                prev_weed[\"ttl\"] = tracking_ttl\n",
    "                                # Mark current weed as not new\n",
    "                                current_weed[\"is_new\"] = False\n",
    "                                break\n",
    "                        \n",
    "                        # Add to current detections\n",
    "                        current_detected_weeds.append(current_weed)\n",
    "                        \n",
    "                        # If it's a new weed, add its angle to our collection and structured data\n",
    "                        if current_weed[\"is_new\"]:\n",
    "                            angle_info = f\" (Camera: {camera_angle:.1f}°)\"\n",
    "                            all_unique_angles.append(angle_info)\n",
    "                            \n",
    "                            # Add to structured data format (using camera angle as requested)\n",
    "                            structured_data = {\n",
    "                                \"distance\": round(depth, 1),  # Round to 1 decimal place, in meters\n",
    "                                \"angle\": round(camera_angle, 1)  # Round to 1 decimal place, in degrees\n",
    "                            }\n",
    "                            structured_weed_data.append(structured_data)\n",
    "                        \n",
    "                        # Store detection data\n",
    "                        weed_coordinates.append(current_weed[\"coordinates\"])\n",
    "                        confidence_scores.append(confidence)\n",
    "                        weed_depths.append(depth_mm)\n",
    "                        weed_angles.append({\"camera\": camera_angle,  \"vertical\": vertical_angle})\n",
    "                        \n",
    "                        # Draw bounding box\n",
    "                        box_color = \"red\" if current_weed[\"is_new\"] else \"orange\"\n",
    "                        draw.rectangle([x_min, y_min, x_max, y_max], outline=box_color, width=3)\n",
    "                        \n",
    "                        # Draw direction arrow to visualize angle\n",
    "                        arrow_length = 50\n",
    "                        arrow_x = center_x + arrow_length * math.sin(math.radians(camera_angle))\n",
    "                        arrow_y = center_y + arrow_length * math.sin(math.radians(vertical_angle))\n",
    "                        draw.line([center_x, center_y, arrow_x, arrow_y], fill=\"blue\", width=2)\n",
    "                        \n",
    "                        # Draw text information\n",
    "                        status_text = \"NEW\" if current_weed[\"is_new\"] else \"TRACKED\"\n",
    "                        label = f\"Weed: {confidence:.2f} ({status_text})\"\n",
    "                        depth_text = f\"Depth: {depth:.1f}m\"\n",
    "                        angle_text = f\"Angle: {camera_angle:.1f}°\"\n",
    "                        \n",
    "                        draw.text((x_min, y_min-45), label, fill=box_color)\n",
    "                        draw.text((x_min, y_min-30), depth_text, fill=box_color)\n",
    "                        draw.text((x_min, y_min-15), angle_text, fill=\"blue\")\n",
    "            \n",
    "            # Update tracking list: decrement TTL and remove expired ones\n",
    "            updated_tracking = []\n",
    "            for weed in previously_detected_weeds:\n",
    "                weed[\"ttl\"] -= 1\n",
    "                if weed[\"ttl\"] > 0:\n",
    "                    updated_tracking.append(weed)\n",
    "            \n",
    "            # Add current detections to tracking list\n",
    "            for weed in current_detected_weeds:\n",
    "                updated_tracking.append(weed)\n",
    "            \n",
    "            # Update the tracking list\n",
    "            previously_detected_weeds = updated_tracking\n",
    "            \n",
    "            # Convert the PIL image back to OpenCV format for display\n",
    "            display_image = np.array(image_pil)\n",
    "            \n",
    "            # Add status information to the frame\n",
    "            cv2.putText(display_image, f\"Current Weeds: {len(weed_coordinates)}\", \n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(display_image, f\"Unique Weeds: {len(all_unique_angles)}\", \n",
    "                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display the result\n",
    "            cv2.imshow('Real-time Weed Detection', display_image)\n",
    "            \n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"Detection stopped by user\")\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Stop streaming\n",
    "        pipeline.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Stream stopped and windows closed\")\n",
    "        \n",
    "        # Print formatted output for easy reading\n",
    "        print(\"All unique weed angles detected (servo compatible):\")\n",
    "        for i, angle in enumerate(all_unique_angles):\n",
    "            print(f\"Weed #{i+1}: {angle}\")\n",
    "        state[\"weeds\"] = [{'distance': 0.4, 'angle': 1.6}, {'distance': 0.4, 'angle': 5.2}]\n",
    "        structured_weed_data = [{'distance': 0.4, 'angle': 1.6}, {'distance': 0.4, 'angle': 5.2}]\n",
    "        if len(structured_weed_data) > 0:\n",
    "            return \"Weed Detected\"\n",
    "        else:\n",
    "            return \"No Weed Detected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"motor_control\")\n",
    "def motor_control(direction, distance, speed=10):\n",
    "    \"\"\"\n",
    "    Control the robot by making API calls to the robot control server.\n",
    "    \n",
    "    Args:\n",
    "        direction (str): The direction to move ('forward', 'backward', 'left', 'right')\n",
    "        distance (float): The distance to move in meters (ignored for 'stop')\n",
    "        speed (int, optional): The speed of the motors (0-100). Defaults to 10.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The response from the server\n",
    "    \"\"\"\n",
    "    base_url = \"http://192.168.190.134:8000\"  # Change to your server's address\n",
    "    \n",
    "    # For the stop endpoint, we don't need to send any parameters\n",
    "    if direction.lower() == \"stop\":\n",
    "        response = requests.post(f\"{base_url}/stop\")\n",
    "        return response.json()\n",
    "    \n",
    "    # Validate inputs\n",
    "    if direction.lower() not in [\"forward\", \"backward\", \"left\", \"right\"]:\n",
    "        return {\"error\": \"Invalid direction. Use 'forward', 'backward', 'left', 'right', or 'stop'\"}\n",
    "    \n",
    "    if not isinstance(distance, (int, float)) or distance <= 0:\n",
    "        return {\"error\": \"Distance must be a positive number\"}\n",
    "    \n",
    "    if not isinstance(speed, int) or speed < 0 or speed > 100:\n",
    "        return {\"error\": \"Speed must be an integer between 0 and 100\"}\n",
    "    \n",
    "    # Prepare the request data\n",
    "    data = {\n",
    "        \"distance\": distance,\n",
    "        \"speed\": speed\n",
    "    }\n",
    "    \n",
    "    # Call the appropriate endpoint based on direction\n",
    "    endpoint = f\"/{direction.lower()}\"\n",
    "    response = requests.post(f\"{base_url}{endpoint}\", json=data)\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "@tool(\"obstacle_avoidance\")\n",
    "def obstacle_avoidance():\n",
    "    \"\"\"\n",
    "    Capture and analyze image from RealSense camera for obstacles.\n",
    "    Returns:\n",
    "        \"obstacle detected\" - if obstacle is found\n",
    "        \"no obstacle detected\" - if no obstacle is found\n",
    "        \"error: [description]\" - if any error occurs\n",
    "    \"\"\"\n",
    "    image_path = \"captured_images/temp_realsense_image.jpg\"\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "\n",
    "    try:\n",
    "        pipeline.start(config)\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not color_frame:\n",
    "            return \"error: No color frame received\"\n",
    "\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        cv2.imwrite(image_path, color_image)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"error: {str(e)}\"\n",
    "\n",
    "    finally:\n",
    "        pipeline.stop()\n",
    "\n",
    "    try:\n",
    "        img = genai.upload_file(path = image_path)\n",
    "        prompt = '''\n",
    "            Carefully examine the image. Determine if there is a clear, open path that a ground rover can travel through. \n",
    "            If you see any object, terrain feature, or condition that could block the rover’s movement (e.g. rocks, walls, debris, steep slopes), \n",
    "            or if there is no clearly visible path at all, respond with \"obstacle detected\".\n",
    "\n",
    "            If the image shows a clear and navigable path suitable for a rover, respond with \"no obstacle detected\".\n",
    "\n",
    "            Only respond with one of these two phrases: \"obstacle detected\" or \"no obstacle detected\".\n",
    "            '''\n",
    "\n",
    "\n",
    "        response = model.generate_content([img, prompt], request_options={\"timeout\": 600})\n",
    "        result = response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        result = f\"error: {str(e)}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"camera\")\n",
    "def camera(direction: str):\n",
    "    \"\"\"\n",
    "    Control the robot's camera by making API calls to the robot control server.\n",
    "    \n",
    "    Args:\n",
    "        direction (str): The direction to point the camera ('left', 'right', 'straight')\n",
    "    \n",
    "    Returns:\n",
    "        dict: The response from the server\n",
    "    \"\"\"\n",
    "    base_url = \"http://192.168.190.134:8000\"  \n",
    "    # Validate inputs\n",
    "    if direction.lower() not in [\"left\", \"right\", \"straight\"]:\n",
    "        return {\"error\": \"Invalid direction. Use 'left', 'right', or 'straight'\"}\n",
    "    \n",
    "    # Call the camera endpoint\n",
    "    endpoint = f\"/camera/{direction.lower()}\"\n",
    "    response = requests.post(f\"{base_url}{endpoint}\")\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motor(direction, distance, speed=10):\n",
    "    \"\"\"\n",
    "    Control the robot by making API calls to the robot control server.\n",
    "    \n",
    "    Args:\n",
    "        direction (str): The direction to move ('forward', 'backward', 'left', 'right')\n",
    "        distance (float): The distance to move in meters (ignored for 'stop')\n",
    "        speed (int, optional): The speed of the motors (0-100). Defaults to 10.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The response from the server\n",
    "    \"\"\"\n",
    "    base_url = \"http://192.168.190.134:8000\"  # Change to your server's address\n",
    "    \n",
    "    # For the stop endpoint, we don't need to send any parameters\n",
    "    if direction.lower() == \"stop\":\n",
    "        response = requests.post(f\"{base_url}/stop\")\n",
    "        return response.json()\n",
    "    \n",
    "    # Validate inputs\n",
    "    if direction.lower() not in [\"forward\", \"backward\", \"left\", \"right\"]:\n",
    "        return {\"error\": \"Invalid direction. Use 'forward', 'backward', 'left', 'right', or 'stop'\"}\n",
    "    \n",
    "    if not isinstance(distance, (int, float)) or distance <= 0:\n",
    "        return {\"error\": \"Distance must be a positive number\"}\n",
    "    \n",
    "    if not isinstance(speed, int) or speed < 0 or speed > 100:\n",
    "        return {\"error\": \"Speed must be an integer between 0 and 100\"}\n",
    "    \n",
    "    # Prepare the request data\n",
    "    data = {\n",
    "        \"distance\": distance,\n",
    "        \"speed\": speed\n",
    "    }\n",
    "    \n",
    "    # Call the appropriate endpoint based on direction\n",
    "    endpoint = f\"/{direction.lower()}\"\n",
    "    response = requests.post(f\"{base_url}{endpoint}\", json=data)\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"spray\")\n",
    "def spray():\n",
    "    \"\"\"\n",
    "    Spray the detected weeds with herbicide by moving to average distance,\n",
    "    adjusting spray angle for each weed, and activating the spray motor.\n",
    "    Returns:\n",
    "        str: A message indicating the result of the spraying operation.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "    'weeds': [{'distance': 0.4, 'angle': 1.6}, {'distance': 0.4, 'angle': 5.2}]  # or True, depending on current analysis\n",
    "}\n",
    "\n",
    "    if not state['weeds']:\n",
    "        return \"No weeds detected. No spraying needed.\"\n",
    "    \n",
    "    try:\n",
    "        # Calculate average distance\n",
    "        total_distance = sum(weed['distance'] for weed in state['weeds'])\n",
    "        avg_distance = total_distance / len(state['weeds'])\n",
    "        \n",
    "        # Move to average distance\n",
    "        motor_response = motor('forward', avg_distance, speed=10)\n",
    "        if 'error' in motor_response:\n",
    "            return f\"Movement failed: {motor_response['error']}\"\n",
    "        \n",
    "        base_url = \"http://192.168.190.134:8000\"\n",
    "        \n",
    "        for weed in state['weeds']:\n",
    "            # Convert angle to degrees if in radians (assuming angles are in radians)\n",
    "            angle_deg = math.degrees(weed['angle'])  # Remove if angles are already in degrees\n",
    "            \n",
    "            # Adjust sprayer angle\n",
    "            turn_response = requests.post(\n",
    "                f\"{base_url}/turn_spray\",\n",
    "                json={\"angle\": angle_deg}  # Use weed['angle'] if already in degrees\n",
    "            )\n",
    "            \n",
    "            # Activate spraying mechanism\n",
    "            spray_response = requests.post(f\"{base_url}/activate_spray\")\n",
    "            \n",
    "            if turn_response.status_code != 200 or spray_response.status_code != 200:\n",
    "                return f\"Spraying failed at weed {weed}\"\n",
    "\n",
    "        return \"Spraying operation completed.\"\n",
    "    except Exception as e:\n",
    "        return f\"Spraying error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_template = '''\n",
    "### **Role**\n",
    "You are an intelligent weed detection and spraying agent. Your job is to detect weeds, spray them, and move. You operate autonomously using the tools below.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tools**\n",
    "\n",
    "1. **analyze_image()**  \n",
    "   - Detects weeds and returns:\n",
    "     - `str`: \"weed detected\" or \"no weed\".\n",
    "   - **Always call this first** before any movement or spraying.\n",
    "\n",
    "2. **spray()**  \n",
    "   - Sprays weedicide.\n",
    "   - **MUST be called immediately** when `analyze_image()` returns \"weed detected\".\n",
    "   - Returns: `\"Spraying done\"`\n",
    "\n",
    "3. **motor_control(distance, direction, speed=10)**  \n",
    "   - Moves the robot.\n",
    "   - Inputs:\n",
    "     - `distance (float)`: in meters.\n",
    "     - `direction (str)`: `\"forward\"`, `\"backward\"`, `\"left\"`, `\"right\"`.\n",
    "     - `speed (int)`: Optional, default = 10.\n",
    "   - Use weed depth for movement when weed is detected.\n",
    "   - If no weed, check for obstacles first.\n",
    "   - NOTE THAT THE DISTANCE THAT U USE SHOULD ALWAYS BE GREATER THAN 0.5 \n",
    "\n",
    "4. **camera(direction)**  \n",
    "   - Moves the camera: `\"left\"`, `\"right\"`, or `\"straight\"`.\n",
    "\n",
    "5. **obstacle_avoidance()**  \n",
    "   - Returns `\"obstacle detected\"` or `\"no obstacle detected\"`.\n",
    "   - Use this when no weed is detected.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Workflow**\n",
    "\n",
    "1. **Detect Weeds:**  \n",
    "   - Call `analyze_image()`.\n",
    "\n",
    "2. **If `analyze_image()` returns \"weed detected\":**  \n",
    "   - **You MUST immediately call `spray()`**  \n",
    "   - Wait for `\"Spraying done\"`  \n",
    "   - Then: `motor_control(1.0, \"forward\")`  \n",
    "   - Repeat the loop\n",
    "\n",
    "3. **If `analyze_image()` returns \"no weed\":**  \n",
    "   - Call `obstacle_avoidance()`  \n",
    "     - **If \"no obstacle detected\":**  \n",
    "       → `motor_control(1.0, \"forward\")`\n",
    "     - **If \"obstacle detected\":**  \n",
    "       → `camera(\"left\")` → `analyze_image()`  \n",
    "         - If \"weed detected\":  \n",
    "           → `motor_control(0, \"left\")`  \n",
    "           → `spray()` → `\"Spraying done\"`  \n",
    "           → `motor_control(1.0, \"forward\")`  \n",
    "         - If \"no weed\":  \n",
    "           → `camera(\"right\")` → `analyze_image()`  \n",
    "             - If \"weed detected\":  \n",
    "               → `motor_control(0, \"right\")`  \n",
    "               → `spray()` → `\"Spraying done\"`  \n",
    "               → `motor_control(1.0, \"forward\")`  \n",
    "             - If \"no weed\":  \n",
    "               → `obstacle_avoidance()`  \n",
    "                 - \"no obstacle\": `motor_control(0, \"right\")`  \n",
    "                 - \"obstacle detected\": `motor_control(0, \"left\")`\n",
    "\n",
    "4. **Repeat the Loop Continuously**\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Examples**\n",
    "\n",
    "**Weed Detected:**  \n",
    "→ `analyze_image()` → returns \"weed detected\"  \n",
    "→ **Must call** `spray()` → `\"Spraying done\"`  \n",
    "→ `motor_control(1.0, \"forward\")`\n",
    "\n",
    "**No Weed, No Obstacle:**  \n",
    "→ `analyze_image()` → \"no weed\"  \n",
    "→ `obstacle_avoidance()` → \"no obstacle\"  \n",
    "→ `motor_control(1.0, \"forward\")`\n",
    "\n",
    "**No Weed, Obstacle:**  \n",
    "→ `camera(\"left\")` → `analyze_image()`  \n",
    "→ If \"weed detected\": `motor_control(0, \"left\")` → `spray()` → `motor_control(1.0, \"forward\")`  \n",
    "→ If \"no weed\": `camera(\"right\")` → `analyze_image()`  \n",
    "→ If \"weed detected\": `motor_control(0, \"right\")` → `spray()` → `motor_control(1.0, \"forward\")`  \n",
    "→ If \"no weed\":  \n",
    "   - `obstacle_avoidance()`  \n",
    "     - \"no obstacle\": `motor_control(0, \"right\")`  \n",
    "     - \"obstacle\": `motor_control(0, \"left\")`\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Note**\n",
    "- Always start with `analyze_image()`\n",
    "- **If it returns \"weed detected\", you MUST call `spray()` immediately**\n",
    "- Never skip spraying if weed is found\n",
    "- If no weed, check for obstacles and proceed cautiously\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            configuration = config.get(\"configurable\", {})\n",
    "            user_id = configuration.get(\"user_id\", None)\n",
    "            state = {**state, \"user_info\": user_id}\n",
    "            result = self.runnable.invoke(state)\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Please provide a proper plan\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            planner_template\n",
    "            \n",
    "        ),\n",
    "        (\n",
    "            \"placeholder\", \n",
    "            \"{messages}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner_tools = [\n",
    "    analyze_image,\n",
    "    motor_control,\n",
    "    camera,\n",
    "    obstacle_avoidance,\n",
    "    spray\n",
    "]\n",
    "\n",
    "planner_runnable = planner_prompt | llm.bind_tools(planner_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1100):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"agent\", Assistant(planner_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(planner_tools))\n",
    "\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "planner_graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAAAXNSR0IArs4c6QAAIABJREFUeJztnWdcFNf+xs9sZTu9dxAEUVQsEYxdY4uIBQsmdm8sNyFGk5jcxMSLxhtzjbEk1mgMKpYgxnLFht3EgoUmIEgvy1K2L9vm/2L9o9ksiLizZ5Y9348vdndmzu9Z9vHMmVN+B8NxHCAQsKHAFoBAAGREBFlARkSQAmREBClARkSQAmREBCmgwRbQEZqVuvoqtUKqU0i1Wi2uVVtBDxSTRaExMDaPxuZT3XzsYMshHdZkRLlEU5gpL86WSeo1PEc6m0dl82h8Rzqwhq5QvQ7UljQrpHI6k1L2WBEQwQnszgnszoWtiyxgVtGhrdfhN0/Wi6qanTwZgRFcr2AWbEWvhUqhe5otryhUVBWrosc7denFg60IPlZgxJw/xJeP1kW/7dRriANsLWZGUq+5eaq+WaEb9Y47i0uFLQcmZDfi5aNCOzbljXHOsIUQiKi6OW1b5ejZ7t5d2LC1QIPURjyfXOseYNc9RgBbiCU4vq3yzThnZ08mbCFwIK8R036sDO7JjYi2CRcaOL6tonuMfXBPW3yCIWk/4rW0Ov9wjk25EAAQt9T7j//VN9aqYQuBABmNmJ8ppdEpPYfYwxYCgYRPfTOOCkl7myIOMhrxytG63sNs0YUAAAzD/MM5N0/WwxZiaUhnxHsXGiNi+EyW7fZl9B7mkPunRCXXwRZiUchlRBzHy/IV0eM7c2dNexg0yeXBlSbYKiwKuYxYnCVnssglCQq+oezsm2LYKiwKuX71p9nygAiOhYN+8sknJ0+e7MCFI0aMqKqqIkARYHGp9s6M6hIlEYWTE3IZsalOE9jd0kbMy8vrwFU1NTVNTQTePUP6cMsLFMSVTzZIZESVXNcoVBP3mJKWlhYfHx8TEzN8+PCVK1fW1tYCAPr06VNVVfX1118PGTIEAKDT6bZv3z5x4sTo6OgxY8asX79eqXxWLY0YMeLgwYPvv//+gAEDrl27Nn78eADAhAkTPvroIyLUcvg0UYUtdSjipEFUpTqwvpSgwjMzM6OiolJTU8vLy7OyshYsWDBnzhwcx2tra6OiolJSUpqamnAc379/f//+/dPT00tLS2/dujV69OgNGzYYSnjrrbcmT578ww8/PHz4UKlUnjt3LioqKi8vTyaTESG4+qnyyPdlRJRMTkg0H1Eu0XH4RFWHRUVFTCbz7bffptFo3t7e69evr66uBgAIBAIAAJvNNrwYM2bMgAEDgoODAQC+vr6jRo26ceOGoQQMw+zs7N5//33DWw6HAwDg8/mGF2aHI6DKxTbUg0MiI+J6nEHYI3OfPn0wDFuwYEFsbGz//v09PT2dnJz+fpq9vf3p06eTkpKEQqFWq1UoFGz28xkxPXr0IEje36HSMIYdiRpOREOir8rm08R1GoIK9/f337t3r7e395YtWyZMmDBnzpzs7Oy/n7Zhw4bdu3fHx8fv2rXr4MGDcXFxLx7lci03HUHWpKXSMIuFgw6JjMjhU+USAm9GXbp0SUpKOn/+/I4dO6hUamJiolr9l6cBnU534sSJ2bNnjx071svLy9nZWSaTEaenbQhtqJAQEhmRzaM5utP1ekLG+7Ozsx89egQAoFKpUVFRixcvbmpqqq9/NqRrmGSg1+t1Op2hsQgAkMvlV69ebXv+AXGzE5oVOhcfG5qbSCIjAgDs2NTiLDkRJd+8eXP58uUXL16sqKjIz89PSUnx8PBwd3dnMplMJjMzMzM/Px/DsNDQ0FOnTlVUVBQWFiYmJsbExEgkkpKSEq1Wa1Qgn88HAFy/fr24uJgIwfn3pB7+1r0055UglxH9u3FKcggx4rx58+Li4jZt2jRlypSlS5fiOL5582YMwwAAc+bMuXDhwpIlS5RK5ZdffqnT6eLj41etWjV9+vSlS5e6u7u/++67QqHQqMCwsLDo6Ojvv//+22+/NbtanRavfKL07WpDKwfINUNbKdOeS66Nfc8LthDIPM2RlRcoB8W5wBZiOchVI7K4NAc3xkMbm3jyd27+Xm9rs9NJ1I9oIOZt5x2fFkUONj0xVqfTDR8+3OQhtVrNYDBMHgoICNi7d69ZZT5n3759+/btM3mIy+W29twdFhb2008/mTz0+K7E1cfO0c30d+mskOvWbODBlSYMwyMHmV7FLJVKTX7e3NzMYDAMzT4jKBQKQeMfhrhG3UAtaDQaOp1u8hCVSn2xq/xFTu2uGjzFhWdv+sLOChmNaPgxur0hsPyUMOjY7BcnVxuxhfELPK+m1tXXNMMWYlEuHRa6+9vZoAvJWyMahp4P/7d80CQXzyCb6E7LOCL07sKy2Tw4JK0RAQAYBZu+0vfWmfq82xLYWohFr8OPb6t0dGfYrAtJXSO2cPOUqCxPEf22c6fs4L1zriH/rnTIVBdbTnxjHUYEANRVNt88KeLwaZ5BrIAIDotj9bMBhOWqsnzF3XONPYfY9xvtSKHY0EQbk1iHEQ1UFCry70qfZstdfJgCZzqHT+PwaWw+Va+HrawdUDEgbtDIxToc4I/vSDl8WnAkp8cgezqDvK0jS2JNRmyh+qlSVKmWS7RyiZaCYQqZOSePKRSK0tLSsLAwM5YJAOA50HEc5wioPEe6dxCLIyDdUAJcrNKIhJKXl7d27drk5GTYQmwLdF9AkAJkRAQpQEY0BsMwX19f2CpsDmREY3AcLysrg63C5kBGNIElV+shDCAjmgDi4j2bBRnRGAzDnJ1tPUGj5UFGNAbHcZFIBFuFzYGMaAyFQgkICICtwuZARjRGr9c/ffoUtgqbAxkRQQqQEY3BMKwl6wjCYiAjGoPjuFhsW4nUyQAyogns7W10uyGIICOagNAs7QiTICMiSAEyojEYhnl52XoWKMuDjGgMjuOVlZWwVdgcyIgIUoCMaAyGYX5+frBV2BzIiMbgOF5aWgpbhc2BjIggBciIxqDZN1BARjQGzb6BAjIighQgIxqDlpNCARnRGLScFArIiAhSgIxoArSu2fIgI5oArWu2PMiIxlAoFG9vb9gqbA5kRGP0en1FRQVsFTYHMiKCFCAjGoNhmKOjI2wVNgcyojE4jjc0NMBWYXMgIxpDoVD8/f1hq7A5kBGN0ev1JSUlsFXYHMiIxqAaEQrIiMagGhEKyIjGUCgUV1dX2CpsDrThzzNmzJghk8kwDFOr1TKZzMHBAcOw5ubm9PR02NJsAlQjPmPMmDFCobCqqkokEqlUqurq6qqqKh7PdvettTDIiM+YPn26j4/Pi59gGDZ48GB4imwLZMRnMBiMiRMnUqnPN+D19fWdMmUKVFE2BDLic+Lj41uy3mAYNnToUA8PD9iibAVkxOcwGIzJkycbKkVfX9+pU6fCVmRDICP+hfj4eE9PT0N16ObmBluODWGV21frdXhTnUZcryGi6yl25KLLly8P7D25OFtu9sLpDMzJg8HmWeWfnVCsrx8x77Yk5w+JSqZzD2ApJObcu94CsHjU0jy5u5/dsGkuyI4vYmVGzPlDUpwlHzTFnULBYGvpOI01zVdTa+KWenH4yIvPsKY2YkGmtOiRfEi8h1W7EADg4M4cM8/7wDdo9fRzrMaIOI5n3RBHT+gko8AMO2rkEMd7FxthCyELVmNEpUzXKNQwWdR2nGsd8Bzo1cVK2CrIgtUYUdKgdfWxg63CnAic6FqNNTXQCcVqjIgBoJRqYaswJ3o9sLqnfuKwGiMiOjfIiAhSgIyIIAXIiAhSgIyIIAXIiAhSgIyIIAXIiAhSgIyIIAXIiAhSgIyIIAXIiAhSgIxoHo6nHVn/7VewVVgxyIjmoaAgD7YE66Yzr5nQ6XT7f9118eLZOpGQzxfERA/+x6IPWCwWAECr1f7408YLF8/qdNpBbw6PiR78xeoVqcfOOTg4arXa5AN7LmWcq62tdnFxmzolIXbCs3wPcZNHvpMwv1ZYcykjXalUdO/ea8Xyfzk5OScuX/TwYSYAID391MkTl9F+QR2gM9eIx347ePDQvnnzluzZlfLxytU3bl7Z/fO2lkMnT6UuWvjPn7btd3Z22b7zB0NCOgDA9h0/HD7ya8KMuXt2H546JWHrtu9On0kzXEWj0Q4d/sXfP/DQgZM/7z5SWPj41+TdAICkNRtDunQdNnRUWuoFDocD9UtbK525RhwxfEzfPgMCA4MBAN7evkOHjPrz9g3DofRzpwbGDBk/Lg4AMH/ektzcrMrKcsOeUyd+P5owc+5bb40HAHh7+RQWPj54aN+4sRMNF/r5BowZPQEA4Orq1q9vdH5+rmHLNCqNRmcwBAJ7qN/YiunMRhQI7M+dP/3dxiSRSKjVapVKBYvFNqzDqqgoGz82ruXMgQOHZt6/AwAoKirQarV9ot5oORQZGXX6TJpCoWCz2QCAwMAuLYd4PL5EKrH41+qcdGYjbtm64fyFMx9+sKpbRCSTwTyU8suljHQAgFwu12q1LDa75Uw+X2B4oVDIAQAffvQPDHu2YtWw7ruhsd5gRCaT+WII617WSiY6rRH1ev2Z/514Z9aCkSPHGj6Ry59t9Uin0wEAKpWq5WTp/1dsHA4XAPD5Z0mBAcEvlubqgvLgEEtnNqJOp2up6uRy+c1bVw2PI0wm09XV7XF+TsvJ169nGF4EBnah0+mNjQ2+g59tLNDU1IhhGIPBeGlE68qZQTY67VMzjUbrEhyafu5UZVVFUVHhZ/9K7N8/RiqVlJWVaLXawYNGXLly4VLGucqqin2/7KgTCQ1Xcbnc8eMn7ftlx6WMc1XVlfcf3F3x8ZL29FTzuLwnT/ILn+RrtZ1qqaHF6LRGBACsXPGlXqebNz9+TdKqSXHTF8xb6ubqvnjpu3Ui4dw57w16c9iG79YsXTZHKpPOmjkPAECj0QEAS977cGLs1J27Ns+eM3n9f1Z3j+j5+aqkl8aKi5suEtW9/8H8lgYA4pWwmiRMtaWqy8fqxi7wace5L0er1cpkUnt7B8Pb/b/uTj2ekpZ6wSyFt5MmofrabzUzP/W1ZFDS0plrxDY4cHDvzFkTLl+5UFlVcf3G5dTjKW+NGg9blE3TaR9W2iZh5ly1unn7jk0NDfWuLm7jxk58952FsEXZNDZqRBqNtnDBsoULlsEWgniGjd6aEWQDGRFBCpAREaQAGRFBCpAREaQAGRFBCpAREaQAGRFBCpAREaQAGRFBCqzGiFQa4DrSYaswJ3ocd3B/+XxbG8FqjOjkyXz6qFNN9RNVqhh2VvP3Jxqr+UNgGBYSxaspVcAWYjYaq9UB3djtONEmsBojAgCGxbtcO1arUnSGTXLuXRDRGCCwO8oJ8QyrmaFtoFmp259U2muYE9ee7uDKsCrtwLDleV2lSlShpDOwQZNcjh07NmXKFNiiSIGVGdHA7u8y2Jg3y44tFmnMXrhep1NrNHZ2hOz75+zJpDOxoB7c4J5cAMDdu3c///zz9PR0ImJZGbi1UVpaumnTJuLK/+qrr4YNG3br1i3iQryIRCLBcTwrK8sy4UiLNbURxWJxfn6+QCD44IMPCAqRm5v78OFDsVh88OBBgkIYwePxDMtYx40bJ5fLLROUhFiNEUUiUVxcXEBAgEAgIC7KoUOHysrKAAAFBQU3btwgLpAR/v7+e/bsKSoqEovFFgtKKqzDiEKhsKys7NKlS+3JuNBh8vLyMjMzDa9FIpHFKkUD7u7uPXr0wDBs2rRpCkXn6aVqJ1ZgxOXLl+M43rt3b6IDHThwoLa2tuVtbm6uJStFA3w+f+3atXfu3LFwXOiQ2og4jt+7dy82NtbNjfAcSLm5uS3VoQGxWJycnEx03L8THBw8ePBgAMDixYvVarXlBUCBvEa8f/++XC7v3r274Vchmv3799fW1ur1+pbnOADA48ePLRC6NRYsWLB48WKIAiwK1Gf2VsnKypo/fz6U0Lm5uQkJCVBCt8aZM2dgSyAcktaIjY2Nu3fvhhXdz88PVmiTuLq6vvPOO7BVEAvpjPjhhx8CAN58801YApRKpVAohBXdJFFRUf/+978BAOXl5bC1EAW5jHj06NG4uLh2nEggSqXSxcUFroa/4+/vDwAoKyv7/vvvYWshBHIZcejQoYMGDYKrQSQSETTQ/PrExMS4uLiUlJTAFmJ+SGFEtVo9ZMgQAICzszNsLUAsFnt5ecFW0SqzZs1yc3PLycl5scuzE0AKI+7bt+/y5cuwVTyjqKjIAt2WrwOLxQoLC5s7d25TUxNsLWYDshF1Ol1tbe2iRYvgyjDC0CAjMxQK5cyZM6WlpZ1mbBqmESUSyYgRI8hW/Zw5cyY8PBy2inYRGRmp0Wj27NkDW4gZgGZEw/BdRkYGLAEmefz48YABAwy7YFgFzs7Ozc3NxcXFsIW8LtD+4rm5uYYHFFJx8+bN0NBQ2CpejSVLlhjth2WNwDHijBkz6HR6yzZj5OHatWsQ+9I7jJeX19mzZ3fs2AFbSMeBYMR79+5t3LgxJCTE8qHbRiwW8/n8Hj16wBbSEUaPHt2zZ8+zZ8/CFtJBLL14SqvVYhhGpVItGbSd/Pzzz0qlcunSpbCF2CIWrRHz8vLmzJlDThcCAFJTUydNmgRbxeuyadOmixcvwlbxyljUiBkZGdu3b7dkxPZz48aNvn37enh4wBbyuiQmJubn51dUVMAW8mpY5bpmIpg2bdratWuDg4PbcS7C/FioRpRKpR9//LFlYnWA8+fPBwQEdCYX5uXlbd26FbaKV8BCRtyyZUv//v0tE6sD/PDDDytWrICtwpyEhYXR6fTTp0/DFtJeLHFr1ul0IpGIbEN5LWzevFkgEMyePRu2EJvGEjUijuOOjo4WCNQBSkpK7ty501ldWF1dnZWVBVtFu7CEEefPn5+fn2+BQB0gMTFx3bp1sFUQhYeHx+rVq0tLS2ELeTmEG1EsFjOZzIiICKIDdYCkpKTZs2f7+JhnM3Jysnnz5qqqKtgqXo7tdt9cvHjxzz///Oyzz2ALQQBL7Nfc1NREo9G4XHKlRi0rK9u6devx48dhC7EEJ06cUKlU06ZNgy2kLQi/Na9fv/7WrVtER3lV4uPjjxw5AluFhYiOjt67dy9sFS+BcCPyeDyyzbxftWrVvn376PROtVlGG7i4uKSkpJA8jY7NtRFXrlw5ZsyYYcOGwRaC+AuE14gVFRVarZboKO1kw4YNUVFRNujCsrKyhIQE2CragnAjfvLJJ0+ePCE6Sns4duyYm5vb9OnTYQuBgK+vr0wma2xshC2kVQg3Ynh4uE4Hf2eUw4cPFxcXv/vuu7CFQOPEiRMODg6wVbSKTbQRf//99/v3769evRq2EJgolUocx9lsku51RXiN2NTUBDchwdmzZ+/cuWPjLgQAXL9+fc2aNbBVtArhRrx79+4333xDdJTWOHbs2NWrVw053WwcPz+/mpoa2CpahfBbs1AonDx5skAgkEqlUqnUKE81oSQnJ/N4vNjYWItFRHQYoob4Fi1a9OjRo5aOG6VSach8mpmZaYH9AQxt88LCwq+//toCsayFhoYG0s7HI+rWvHPnzr/PamEymZZZNfzrr78WFRUhFxoxY8YMkUgEW4VpCGwjLlu2zNPTs+UtjuPh4eE0GuHTLJKTk+vr65cvX050IKvDyclJpVLBVmEaAo04ePDg8ePHczgcw1s7OzsLLFvZuHEjhUJJTEwkOpA1cvDgQW9vb9gqTEPsU/OiRYv69etnSK7l4ODQvXt3QsOtWbPGzc1t5syZhEaxXsgwstAahHffrFu3LigoSK/XCwSCoKAg4gJ9+umnkZGRJB9RhcvcuXNzcnJgqzBNu1psWo1eKdN3NAT28fLV69at69srRtpI1OyH1V+uHjNh+MiRIwkqv3MQERFB2gR2L+lHzLsteXRN3FCjZnFJmrDG8BjE4Ogbq/CACE7vYfYeASzYishF7969MQzDcbwlDyCO4yEhISkpKbClPaetGvH2uQZRlebNSe48RyuYQ4rjuLhOc/m32uhxTn5hJB1RhUJoaGh+fv6LaXC5XO7ChQuhijKm1Tbin2cbxHXaN+PcrMKFAAAMw+xdGeMX+vx5tqE0z+b2O26D6dOns1h/uUv4+fkNHz4cniITmDZio1Atqmx+Y7yrxfWYgeEJHvczyDvxzvLExsa+uHMMm82eO3cuVEUmMG1EUWUzjpMur3A7YTCpTXUaSYMGthASkZCQwGAwDK8DAwOHDh0KW5Expo0oE+tcfEi6DVh78AnlNAqREZ8TGxtr6MrmcDhz5syBLccEpo2oadZrVB3ur4GPrEmD6zr/hN9XIiEhgU6nBwYGknAzB0sssEd0gNLHcmmjViHRqZV6ldI8wyEc8MaQbv/s1q3bhUPm2cSPw6fpdTiHT+Pwqe4BdjyH13qoRUYkEfl3JQX35aW5cs8QvkaDU2lUKp0GKGbrteg3YBwAQGqmHgW5CtOqNfoyNa7HJakiFoca3JPTLZrPFXREMDIiKSi8L72WVu/gyaEyOd1GupBwB5q2ce0ClNLm8qeK3NtVAeHsgROdaPRXGz1GRoSMToef3lMjlwLvSA8Gy4p/DhaPyeIxnQMcGsrFO1c9HTLVJbw/v/2XW/E37wQIy1VHN1UE9ffk+5B0CLgDOPoIHH0EWbfq6iqbB09yaedVVrP7YedDXK8+s1fYbUSAHa/zuLAFt1CXehHlWlp9O89HRoRDTakq7cca/75e7TjXWnH0sRfWgP/90q6lg8iIENBq9KlbKv36dGYXGnDys1fIKXcvvHzEFRkRAqd/rg16o/O70IBTgFNpfnN5obzt05ARLU3OLbFcjjE51jGnySywnflXfntJYxEZ0dLcONngGkjSxcUEweIzKTRa4X1pG+eQyIirv/r4oxWLYasgluybYic/Ho1J0unuD7Mvrviiv1xu/lxFTgGOOX/I2jjBbEY8nnZk/bdfmau0zsrjuzImx4qnNXUYJpveUKNurG01fbLZjFhQkGeuojormmZ9XbmK62SjS2o4zuzirFYrRfOMrCQuX/TwYSYAID391M4dB7oEh2ZlPdi1Z2tBQR6GYWFdIxYu/GdY126Gk0+fSTtyNLmqqoLFYvfvF734vQ8dHZ2MCjx9Ju3YbwerqyuZTLvIHr2XLV3h6krSrfzaT0me3DmAR1z59x+du3LjYG3dUyaT3av7qDEjFjMYdgCA/SmfYRgI7TIg4+p+sbTO1dkvbvwKP5/uAACdTnvizPeZj87ien146MDgwD7EyeO5sGvKWm0mmqdGTFqzMaRL12FDR6WlXggMCC4vL13x8RIXZ9dtW/Zt3byXxWavWLlYKKwFAJw7d/q7/yaNGjnu592H13y1oaDw8arPPjBaSfjo0f3v/ps0edKMPbsPf7PuB7Gk6et/f2oWnXAR12l1GqJmM2TnXjlw9IuQ4H4fLU2eFvfFo5xLx35/lg2QSqU9LX1YVp6TuGT/V5+cZbMFh1OTDIcuXf3lz7tpE8Ykfrhkf4B/zwtXfiZIHgCAzqRVFytbO2oeI3K5XCqNRmcwBAJ7KpV64vdjLBZ71adrgoK6BAV1+XxVklarTT93CgBw9NiBmJjBCTPn+vj49ewZ9c9lKwsKH2dnP3yxtKclRUwmc/Rbb3t5eoeHRaz+Yv3SJR+ZRSdcZE1a4h5TLl3bH+jfe+zIJc5OPmEh0eNGLc18eLZJ/GzqoVqtnDAmkclgMRh2vXuMFopK1GoVAODew/9FhA/u1/ttZyef6H6TQ4IIzAlDt6Op5K3OrSTkqbmgMC+kS9eWfEtsNtvHx6+oqECr1RYVF4aHPU88EhoaDgB4UlTw4uW9evbBMOz9xAWnTh+vrqlydHQKDyPjVn6vikKmI8iIer2+oiovJLhfyyeB/r0BANU1z9LoOzv5GG7TAAA2iw8AUCglWq1GVF/u4xXecpWvdzci5LXA5FDlEtNLOAiZfaNQyJ0cnV/8hM3mKBRypcqQxpnz/HMWGwCgVP5lrqavr//WzXsPHf5l564t0o1rw8Iili1d0Qm8SFxKVI1Gpdfrzl3adT5jz4ufS6TPktDRaH+fV4Gr1UoAAP2FQ0wmsevBcR3e2lRLQozI4XDl8r88H8nlMidHZ5Ydi0KhKBTPR3vkCrnhfKMSgoK6/OuzJJ1Ol5X1YM/eHz/7PPFIypmWdWhWCldArasjJA0SnW5HpdIGvjGtf9SEv0TktNVzTmfYAQCUzc9/KaWyrT7n1wTHcbVKz+aZtpw5b80tzxyhIeH5BXkazbNKWCqTlpWVdO3ajUajBQeFZGU/aLkkN+dRyw26hby87JycRwAAKpXas2fUvLmLxeKmhob2TigiLVx7mlZNiBEpFIqXR9fGpmpXF3/DP0cHLwqFxma3NTWVTmM42HtU1xS2fFJQdJsIeQa0zTo7TqstE7MZkcflPXmSX/gkXyxuio2d2tys+va7NeXlpcXFT5LWfs7hcN8aNR4AMHXqrD/+uH7kaHJNTfX9B3e3bPsuMrJ3178a8c/bNz//YvmVqxcrqyoKn+Snpqa4u3m4ubmbSyos7F3oNCpRayOHDJyVlZtx6eovwrrSyqr8g8dWb9u9SKV6yVSDXt1HZede+eNuWnXNkys3DlRVF7R9/uugVmo9AlvtQzXbrTkubvo36798/4P5X3+1oV/fARv+s23n7i0LFs2gUqndI3p+/98d9vYOAIARw0c3N6uOHE3etXsrh8MdGDPkH//4wKioWQnztFrN9u2bRPV1HA43IiJy/TebrW4Zx9/x78Y5+0uNc6BzO859ZXp0Gzpj8tcZ1/anX9xpZ8f19+2xeN6Pdnactq8aOWyBXNF06uxmPa4PC4kZN2rZ/sOr9Dgh/1vkInmXHq1OATadDex2eoNaBSKHWOvY/KVDVZFvCvy7veRnsDzHt1XR+Dyesy3miCq6WT4l0UvgZHraEYkmPdgCXftxm2XNsFVAQCVTO3szW3MhWjxlacL68m+dKuG7cRks0z9Jdt7VlFTTmyFwWAK5Umzy0BtRE8eP/qe5RD4tfbAn2fQIgl6vo2AlNPAwAAAClklEQVQUYKqZNKDvpHGjlrZWpqi4YeDb9m0ERUa0NG9OdLpzsdGzm+lMayFB/ZYv+dXkIbVa1dIpbQSTac5GiLdnWGsaNJpmKpX+YqrF9miQN6rodNw/vC2RyIiWpksvXuEDuUrabHLxHoNh58jwNHWd5aDTmY4O5tSgapQOnfqSRzTURoTA2Lnuxber9HqbSBNVW1AX2ovl+rLkcsiIcJjxsW/xHxWwVRBObWG9iwclIlrw0jOREeHg4MqY+YlX4fUyndaK0/+1TV1RfVA4fVh8u/IOIyNCg82lT/vIu/B6mbyx1Vl6Vopeq6/MrvEPofUZ4dDOS5ARYcJ3pL/3nyC6Xl7xsFop6ST9i3VPG/Ovlg0cZ9931CsMiKCnZviMmuVWXqC4elzE5DIpDAbfhUPaZX5tIKtXykQKiVAWOch+6pJX3mIMGZEU+ISwEz7xLc2VFzyQF9+udPBgqVV6GoNGZdAwCkkH2SlUikap1ml0ANc3VitdfezCozjhb/i/amZEA8iIJMIvnOMXzgEA1JappI1ahUSrUuibFSTdyZHFxTEKjcNnsvk0jwB3OuO1mnnIiGTEzdfOzRe2CMti2ogMO0wPSHpHaA8cezqFasX6bRDT1SnPgV5XasV9CmV5Mkd3615XYGuYNqKrD9N656EqZVpnLybXHrU6rIlWa0SvYLurv7Ur1yfZuJBc1Xdke/tRESShrf2ac26JCx/IIgc7ObgxqDSyd32rFDqJSH3jhHD0u26uvraY6MiqecnG4U9z5A+uNNU8VVFppL5VC5zpkgaNfzinz0gHB1fUOrQ+XmLEFpqVpB6bx/XAjkP2OhvRBu01IgJBKKgWQZACZEQEKUBGRJACZEQEKUBGRJACZEQEKfg/zsZU4/1PoqEAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(planner_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_printed = set()\n",
    "\n",
    "def askAgent(query: str) -> str:\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"1\"  # Replace with a unique identifier for each session\n",
    "        },\n",
    "        \"recursion_limit\": 100  # Set your desired recursion limit here\n",
    "    }\n",
    "\n",
    "    events = planner_graph.stream(\n",
    "        {\"messages\": [(\"user\", query)]},\n",
    "        config=config,\n",
    "        stream_mode=\"values\"\n",
    "    )\n",
    "\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  analyze_image (call_52pw)\n",
      " Call ID: call_52pw\n",
      "  Args:\n",
      "Loading YOLO model...\n",
      "Model loaded successfully\n",
      "Starting RealSense camera stream...\n",
      "Camera stream started\n",
      "Error: type object 'Image' has no attribute 'fromarray'\n",
      "Stream stopped and windows closed\n",
      "All unique weed angles detected (servo compatible):\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: analyze_image\n",
      "\n",
      "Weed Detected\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  spray (call_gvqf)\n",
      " Call ID: call_gvqf\n",
      "  Args:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maskAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m, in \u001b[0;36maskAgent\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with a unique identifier for each session\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     },\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Set your desired recursion limit here\u001b[39;00m\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m events \u001b[38;5;241m=\u001b[39m planner_graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, query)]},\n\u001b[0;32m     13\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     14\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_print_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_printed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1779\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1779\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\langgraph\\utils\\runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\langchain_core\\runnables\\fallbacks.py:179\u001b[0m, in \u001b[0;36mRunnableWithFallbacks.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    178\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m--> 179\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions_to_handle \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\langgraph\\utils\\runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py:238\u001b[0m, in \u001b[0;36mToolNode._func\u001b[1;34m(self, input, config, store)\u001b[0m\n\u001b[0;32m    236\u001b[0m input_types \u001b[38;5;241m=\u001b[39m [input_type] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tool_calls)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m--> 238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;241m*\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one, tool_calls, input_types, config_list)\n\u001b[0;32m    240\u001b[0m     ]\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# preserve existing behavior for non-command tool outputs for backwards\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# compatibility\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(output, Command) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# TypedDict, pydantic, dataclass, etc. should all be able to load from dict\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch-gpu\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "askAgent(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
